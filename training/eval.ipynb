{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716a342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2600f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer \n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "def get_rougue_score(text, highlights, metric=\"rougeL\"):\n",
    "    score =  scorer.score(text, highlights)[metric].fmeasure\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fea79d",
   "metadata": {},
   "source": [
    "## Prediction extractive _12EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eca3bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predictions_extractive_12epoch.csv\")\n",
    "df['Rouge1'] = df.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge1\"), axis=1)\n",
    "df['Rouge2'] = df.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge2\"), axis=1)\n",
    "df['Rouge3'] = df.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge3\"), axis=1)\n",
    "df['RougeL'] = df.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeL\"), axis=1)\n",
    "df['RougeLsum'] = df.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeLsum\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "888c2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"predictions_extractive_12epoch_with_rouge.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4351d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3420935804294753\n",
      "0.07107610925044244\n",
      "0.022872501832466308\n",
      "0.20376950223567852\n",
      "0.20376950223567852\n"
     ]
    }
   ],
   "source": [
    "print(df['Rouge1'].mean())\n",
    "print(df['Rouge2'].mean())\n",
    "print(df['Rouge3'].mean())\n",
    "print(df['RougeL'].mean())\n",
    "print(df['RougeLsum'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0ffb5",
   "metadata": {},
   "source": [
    "## Prediction extractive my_12EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e19f4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_csv(\"predictions_extractive_my_12epoch.csv\")\n",
    "dfa['Rouge1'] = dfa.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge1\"), axis=1)\n",
    "dfa['Rouge2'] = dfa.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge2\"), axis=1)\n",
    "dfa['Rouge3'] = dfa.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge3\"), axis=1)\n",
    "dfa['RougeL'] = dfa.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeL\"), axis=1)\n",
    "dfa['RougeLsum'] = dfa.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeLsum\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf6b0d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3319439854099592\n",
      "0.06875151021147312\n",
      "0.020799614376231502\n",
      "0.2054400595934515\n",
      "0.2054400595934515\n"
     ]
    }
   ],
   "source": [
    "print(dfa['Rouge1'].mean())\n",
    "print(dfa['Rouge2'].mean())\n",
    "print(dfa['Rouge3'].mean())\n",
    "print(dfa['RougeL'].mean())\n",
    "print(dfa['RougeLsum'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af3fcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.to_csv(\"predictions_extractive_my_12epoch_with_rouge.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a8dfd",
   "metadata": {},
   "source": [
    "# Run below for generating the bert score of the abstract model per column\n",
    "## check the cummulative mean bert_score at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29c192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics, load_metric\n",
    "from statistics import mean\n",
    "metrics_list = list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eca84d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pd.read_csv(\"yale_bart_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd09cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load_metric(\"bertscore\")\n",
    "def bert_scorer(generated_sums, true_sums, metric):\n",
    "    return bertscore.compute(predictions=generated_sums, references=true_sums, lang=\"en\", verbose=False)[metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf08c2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abstract['Rouge1'] = df_abstract.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge1\"), axis=1)\n",
    "df_abstract['Rouge2'] = df_abstract.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge2\"), axis=1)\n",
    "df_abstract['Rouge3'] = df_abstract.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rouge3\"), axis=1)\n",
    "df_abstract['RougeL'] = df_abstract.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeL\"), axis=1)\n",
    "df_abstract['RougeLsum'] = df_abstract.apply(lambda x: get_rougue_score(x['Actual Text'], x['Generated Text'], metric=\"rougeLsum\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb72b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predictions_extractive_12epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb72b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"predictions_extractive_my_12epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21bdc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"bertscore_f1\"] = bert_scorer(df2['Actual Text'],df2['Generated Text'], \"f1\")\n",
    "\n",
    "\n",
    "df2[\"bertscore_precision\"] = bert_scorer(df2['Actual Text'],df2['Generated Text'], \"precision\")\n",
    "\n",
    "\n",
    "df2[\"bertscore_recall\"] = bert_scorer(df2['Actual Text'],df2['Generated Text'], \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "235e5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract[\"bertscore_f1\"] = bert_scorer(df_abstract['Actual Text'],df_abstract['Generated Text'], \"f1\")\n",
    "\n",
    "\n",
    "df_abstract[\"bertscore_recall\"] = bert_scorer(df_abstract['Actual Text'],df_abstract['Generated Text'], \"precision\")\n",
    "\n",
    "\n",
    "df_abstract[\"bertscore_precision\"] = bert_scorer(df_abstract['Actual Text'],df_abstract['Generated Text'], \"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8d18abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.850033\n",
       "1      0.853063\n",
       "2      0.846542\n",
       "3      0.851886\n",
       "4      0.866587\n",
       "         ...   \n",
       "198    0.860166\n",
       "199    0.859972\n",
       "200    0.841402\n",
       "201    0.833093\n",
       "202    0.836516\n",
       "Name: bertscore_f1, Length: 203, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_abstract[\"bertscore_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8d18abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.853757\n",
       "1      0.852930\n",
       "2      0.851437\n",
       "3      0.850306\n",
       "4      0.870103\n",
       "         ...   \n",
       "198    0.863403\n",
       "199    0.860387\n",
       "200    0.844228\n",
       "201    0.840196\n",
       "202    0.837589\n",
       "Name: bertscore_precision, Length: 203, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abstract[\"bertscore_precision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "324d3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cummulative Bert Score F1: 0.8524506658756087\n"
     ]
    }
   ],
   "source": [
    "cummulative_bertscore = df_abstract[\"bertscore_f1\"].mean()\n",
    "print(\"Cummulative Bert Score F1:\",cummulative_bertscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44fac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3383724122981066\n",
      "0.07474964717459603\n",
      "0.021157335557296413\n",
      "0.20339260055258954\n",
      "0.20339260055258954\n"
     ]
    }
   ],
   "source": [
    "print(df_abstract['Rouge1'].mean())\n",
    "print(df_abstract['Rouge2'].mean())\n",
    "print(df_abstract['Rouge3'].mean())\n",
    "print(df_abstract['RougeL'].mean())\n",
    "print(df_abstract['RougeLsum'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "905f5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.to_csv(\"yale_bart_predictions_bert_rouge.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d7bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pd.read_csv(\"yale_bart_predictions_bert_rouge.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e3ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524506658756087\n",
      "0.8505914777957747\n",
      "0.8544527682764776\n"
     ]
    }
   ],
   "source": [
    "print(df_abstract[\"bertscore_f1\"].mean())\n",
    "print(df_abstract[\"bertscore_precision\"].mean())\n",
    "print(df_abstract[\"bertscore_recall\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e3ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8541504021348625\n",
      "0.8527764460723388\n",
      "0.8557632894351564\n"
     ]
    }
   ],
   "source": [
    "print(df[\"bertscore_f1\"].mean())\n",
    "print(df[\"bertscore_precision\"].mean())\n",
    "print(df[\"bertscore_recall\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4e3ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526606627285774\n",
      "0.851575328505098\n",
      "0.8539447851956184\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"bertscore_f1\"].mean())\n",
    "print(df2[\"bertscore_precision\"].mean())\n",
    "print(df2[\"bertscore_recall\"].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
